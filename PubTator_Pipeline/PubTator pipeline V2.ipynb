{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa128cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change_Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907478d3",
   "metadata": {},
   "source": [
    "### Change Log (13/09/2023) ~SIVA\n",
    "\n",
    "#### 1. Shift to Asynchronous API Requests\n",
    "Transitioned from a threaded approach to asynchronous programming with `asyncio` and `httpx` to boost data fetching efficiency. The `workers` parameter remains unused in this version and might be removed or repurposed in future iterations.\n",
    "\n",
    "#### 2. Batch Processing in API Requests\n",
    "Introduced batch processing to reduce the total number of HTTP requests made during data retrieval. This is facilitated by the new `fetch_data_batch_async` function, making data fetching quicker and smoother.\n",
    "\n",
    "#### 3. Refined Error Handling and XML Parsing\n",
    "Enhanced error handling with more robust management of potential XML parsing errors, maintained within the `process_response_batch` function. This enhancement safeguards the robustness of the data fetching process.\n",
    "\n",
    "#### 4. Data Storage and Aggregation Optimization\n",
    "Continued using the `concept_data_dict`, a nested dictionary, for efficient data aggregation and streamlined its population process from batch responses, facilitating seamless data updating and extraction.\n",
    "\n",
    "#### 5. Documentation and Code Readability\n",
    "Updated docstrings and inline comments to reflect recent changes and enhance code readability, assisting in better maintenance and understanding of the code structure and functionalities.\n",
    "\n",
    "#### 6. CSV Writing Logic Adjustment\n",
    "Modified the CSV writing logic to append data in a single instance at the end of the function, reducing IO overhead and optimizing the data writing process.\n",
    "\n",
    "#### 7. Incorporation of Nest Asyncio\n",
    "Included `nest_asyncio.apply()` to ensure compatibility of the asyncio event loop with IPython environments, promoting a smoother development and runtime experience across different Python environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee547d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04a68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your TSV file\n",
    "# Gene-Diesese H.Sapians data from CoCoScore FigShare Bucket\n",
    "file_path = 'Gene_Disease.tsv.gz'\n",
    "\n",
    "# Specify column names (replace with your actual column names)\n",
    "column_names = ['PMID', 'Paragraph No', 'Sentence No', 'Diseas ID', 'Gene ID', 'Text', 'Association_label', 'H']\n",
    "\n",
    "# Read the tab-separated TSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t', names=column_names)\n",
    "\n",
    "df['PMID'] = df['PMID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c51a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import xml.etree.ElementTree as ET\n",
    "import asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "import time\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b529cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "def fetch_pubmed_concepts_df(df, max_pmid_count, workers=4):\n",
    "    \"\"\"\n",
    "    Function to fetch pubmed concepts and convert it to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with a 'PMID' column.\n",
    "    max_pmid_count (int): Maximum number of PMIDs to process.\n",
    "    workers (int): Number of threads to use for parallel requests.\n",
    "\n",
    "    Returns:\n",
    "    str: Summary of the fetching statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Create the CSV file with header if it doesn't exist\n",
    "    if not os.path.exists('pubmed_concepts.csv'):\n",
    "        with open('pubmed_concepts.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"pubmed_id\", \"concept_name\", \"count\", \"gene_id\"])\n",
    "    \n",
    "    # Load existing pubmed_ids from the CSV to avoid duplications\n",
    "    existing_data = pd.read_csv('pubmed_concepts.csv')\n",
    "    existing_pubmed_ids = existing_data['pubmed_id'].tolist()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    api_request_count = 0\n",
    "    \n",
    "    pmid_first_n = df['PMID'][:max_pmid_count]\n",
    "    querylist = pmid_first_n.tolist()\n",
    "    \n",
    "    # Step 3: Remove pubmed_ids that are already in the CSV\n",
    "    querylist = [pmid for pmid in querylist if pmid not in existing_pubmed_ids]\n",
    "\n",
    "    pubtator_api_url = \"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml\"\n",
    "    \n",
    "    response_cache = {}\n",
    "    \n",
    "    concept_data_dict = defaultdict(lambda: defaultdict(lambda: {'count': 0}))\n",
    "    \n",
    "    with tqdm(total=len(querylist)) as pbar:\n",
    "        \n",
    "        async def fetch_data_async(pmid):\n",
    "            nonlocal api_request_count\n",
    "            if pmid in response_cache:\n",
    "                return response_cache[pmid]\n",
    "\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(f\"{pubtator_api_url}?pmids={pmid}&concepts=gene\")\n",
    "                if response.status_code == 200:\n",
    "                    response_cache[pmid] = response.content\n",
    "                else:\n",
    "                    print(f\"Request for PMID {pmid} failed with status code {response.status_code}\")\n",
    "                api_request_count += 1\n",
    "                pbar.update(1)\n",
    "                return response_cache[pmid]\n",
    "\n",
    "        async def fetch_data_batch_async(pmids):\n",
    "            nonlocal api_request_count\n",
    "            pmids_str = ','.join(map(str, pmids))\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(f\"{pubtator_api_url}?pmids={pmids_str}&concepts=gene\")\n",
    "                if response.status_code == 200:\n",
    "                    response_content = response.content\n",
    "                    for pmid in pmids:\n",
    "                        response_cache[pmid] = response_content\n",
    "                else:\n",
    "                    print(f\"Request for PMIDs {pmids_str} failed with status code {response.status_code}\")\n",
    "                api_request_count += 1\n",
    "                pbar.update(len(pmids))\n",
    "                return response_content\n",
    "\n",
    "        def process_response_batch(response_batch, pmid_batch):\n",
    "            for response, pmid in zip(response_batch, pmid_batch):\n",
    "                if response:\n",
    "                    try:\n",
    "                        root = ET.fromstring(response)\n",
    "                        for passage in root.iter('passage'):\n",
    "                            for annotation in passage.iter('annotation'):\n",
    "                                gene_id = annotation.find(\"infon[@key='identifier']\").text\n",
    "                                concept_name = annotation.find(\"text\").text.lower()\n",
    "                                concept_data_dict[pmid][concept_name]['gene_id'] = gene_id\n",
    "                                concept_data_dict[pmid][concept_name]['count'] += 1\n",
    "                    except ET.ParseError as e:\n",
    "                        print(f\"XML parsing error for PMID {pmid}: {e}\")\n",
    "\n",
    "        async def main():\n",
    "            batch_size = 10\n",
    "            for i in range(0, len(querylist), batch_size):\n",
    "                pmid_batch = querylist[i:i+batch_size]\n",
    "                response_batch = await asyncio.gather(*(fetch_data_async(pmid) for pmid in pmid_batch))\n",
    "                process_response_batch(response_batch, pmid_batch)\n",
    "\n",
    "        asyncio.run(main())\n",
    "\n",
    "    concept_data = [\n",
    "        {'pubmed_id': pmid, 'concept_name': concept_name, **data}\n",
    "        for pmid, concepts in concept_data_dict.items()\n",
    "        for concept_name, data in concepts.items()\n",
    "    ]\n",
    "\n",
    "    concept_df = pd.DataFrame(concept_data)\n",
    "    \n",
    "    concept_df.to_csv('pubmed_concepts.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    stats = f\"Stats: \\n1. {len(querylist)} number of pubmed_id parsed with {api_request_count} number of API request\\n2. Total Time: {total_time:.2f} seconds\\n3. Data successfully appended to CSV\"\n",
    "    print(stats)\n",
    "    return \"fetch success\"\n",
    "\n",
    "# Usage example:\n",
    "# df = pd.DataFrame({'PMID': [1, 2, 3, ...]})\n",
    "# result_df = fetch_pubmed_concepts_df(df, max_pmid_count=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed807bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████▉  | 974/1000 [01:59<00:03,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: \n",
      "1. 1000 number of pubmed_id parsed with 974 number of API request\n",
      "2. Total Time: 119.77 seconds\n",
      "3. Data successfully appended to CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fetch success'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_pubmed_concepts_df(df, max_pmid_count=1000, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0626f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##END##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feeeec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f997492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
