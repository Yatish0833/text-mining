{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3r1HIRnYfRd"
      },
      "outputs": [],
      "source": [
        "#Start"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Summmary\n",
        "\n",
        "The goal is to find similarities between sentences containing multiple RSIDs, in order to identify potential relationships between those RSIDs. We starts by loading a subset of tokenized RSID sentences data and the fine-tuned PubMedBERT model. The RSID tokens are also loaded.\n",
        "\n",
        "Sentences containing multiple RSIDs are extracted from the subset. The model generates an embedding for each RSID in these sentences by masking the RSID token and passing it through the model. The embeddings for the different RSIDs within a sentence are aggregated by taking the average. This gives a single sentence embedding capturing information about all the RSIDs present.\n",
        "\n",
        "Cosine similarity is then calculated between these sentence embeddings and all other sentence embeddings in the corpus. Highly similar sentence pairs likely indicate a relationship between the RSIDs in those sentences. A threshold is set for the cosine similarity scores to only keep highly similar pairs. The similarities are sorted and the top most similar pairs for each sentence are kept.\n",
        "\n",
        "The similarities and sentence embeddings are saved to files. T\n",
        "\n",
        "In summary, the key steps were:\n",
        "\n",
        "- Load subset data and model\n",
        "- Extract sentences with multiple RSIDs\n",
        "- Generate embeddings for each RSID\n",
        "- Aggregate embeddings into sentence embeddings\n",
        "- Calculate cosine similarities\n",
        "- Filter using a similarity threshold\n",
        "- Save similarities and embeddings\n",
        "\n",
        "This allows mining the corpus for potential RSID relationships in an unsupervised way by leveraging the pretrained language model. The output data can then be analyzed to surface meaningful relationships."
      ],
      "metadata": {
        "id": "P0HyLrINGA47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GptjSEnCYn1E",
        "outputId": "4f32a356-6853-425b-f302-06867ef5d6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/gdrive/My Drive/Ver S/New/tokenized_rsid_sentences.pkl\" \"./\"\n",
        "!cp -r \"/content/gdrive/My Drive/Ver S/New/trained_model\" \"./\"\n",
        "!cp -r \"/content/gdrive/My Drive/Ver S/New/trained_tokenizer\" \"./\"\n",
        "!cp \"/content/gdrive/My Drive/Ver S/rsid_tokens.pkl\" \"./\""
      ],
      "metadata": {
        "id": "ULDNIkNVYoOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0SzhGraZ3Xm",
        "outputId": "ed71fa9f-66c2-43a9-d0a3-d5808c9b2446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizerFast, BertModel, BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "QZgqZVG-ZPy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenized_data\n",
        "with open(\"./tokenized_rsid_sentences.pkl\", 'rb') as f:\n",
        "    tokenized_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "PtDANjqwZYGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample 1% of the data\n",
        "num_samples = int(0.01 * len(tokenized_data))\n",
        "all_pmids = list(tokenized_data.keys())\n",
        "selected_pmids = random.sample(all_pmids, num_samples)\n",
        "\n",
        "# Create a dictionary with the selected data\n",
        "sampled_data = {pmid: tokenized_data[pmid] for pmid in selected_pmids}"
      ],
      "metadata": {
        "id": "usdYtqWkZaiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./sampled_data.pkl\", 'wb') as f:\n",
        "    pickle.dump(sampled_data, f)\n",
        "\n",
        "print(f\"Sampled {len(sampled_data)} pmid-sentences from the full corpus.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGVlyM0DZtbe",
        "outputId": "4b1b837d-76b5-4bb3-bf38-743291f08e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled 1076 pmid-sentences from the full corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "MODEL_PATH = \"./trained_model\"\n",
        "TOKENIZER_PATH = \"./trained_tokenizer\""
      ],
      "metadata": {
        "id": "uaS3g0bpZuC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "model = BertModel.from_pretrained(MODEL_PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PggDPzHjaOeY",
        "outputId": "ab0ae7b5-ee97-41fb-e671-398d0d39ac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at ./trained_model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(189044, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load RSID tokens and update tokenizer and model\n",
        "with open('./rsid_tokens.pkl', 'rb') as f:\n",
        "    rsid_tokens = pickle.load(f)\n",
        "\n",
        "rsid_token_ids = tokenizer.encode(rsid_tokens, add_special_tokens=False)"
      ],
      "metadata": {
        "id": "ZUgLcQn_cq46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenizer))\n",
        "print(dir(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V3RrrLak1Nx",
        "outputId": "5d9d02d6-b232-46af-eeb1-38611b7d7aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
            "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_added_tokens_decoder', '_added_tokens_encoder', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_batch_prepare_for_model', '_bos_token', '_call_one', '_cls_token', '_compile_jinja_template', '_convert_id_to_token', '_convert_token_to_id', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenize', '_unk_token', '_update_trie', '_upload_modified_files', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'apply_chat_template', 'as_target_tokenizer', 'basic_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'default_chat_template', 'deprecation_warnings', 'do_basic_tokenize', 'do_lower_case', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_vocab', 'ids_to_tokens', 'init_inputs', 'init_kwargs', 'is_fast', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_for_tokenization', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'slow_tokenizer_class', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'tokens_trie', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'verbose', 'vocab', 'vocab_files_names', 'vocab_size', 'wordpiece_tokenizer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sentences with multiple RSIDs\n",
        "sentences_with_multiple_rsids = []\n",
        "rsid_token_ids_set = set(rsid_token_ids)\n",
        "\n",
        "for pmid, values in tqdm(sampled_data.items(), desc=\"Extracting sentences with multiple RSIDs\"):\n",
        "    for input_ids in values[\"input_ids\"]:\n",
        "        rsid_count = len(set(input_ids) & rsid_token_ids_set)\n",
        "        if rsid_count > 1:\n",
        "            sentences_with_multiple_rsids.append((pmid, input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu48kbg4cxn7",
        "outputId": "480a84d0-ff15-4dfa-f7bc-a81b865c4ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting sentences with multiple RSIDs: 100%|██████████| 1076/1076 [00:00<00:00, 40386.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNIf3tm3q55G",
        "outputId": "8878bfc1-2e34-40b7-c77b-ba5996eaea04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MASK_TOKEN = tokenizer.mask_token_id"
      ],
      "metadata": {
        "id": "_nnWTFiZapor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cToIbotRoJe6",
        "outputId": "0abe440a-8f34-4712-d7aa-97e15bb12330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(189044, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyBbV6II24aY",
        "outputId": "bac6ed99-5ca7-4e4a-9832-a883de099cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 22 15:04:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    32W /  70W |  10283MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eNaGf0db6sIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate the embeddings for all the RSIDs within each sentence\n",
        "sentence_embeddings = {}\n",
        "\n",
        "for pmid, input_ids in tqdm(sentences_with_multiple_rsids, desc=\"Generating embeddings\"):\n",
        "    original_sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "    embeddings = []\n",
        "\n",
        "    input_ids_tensor = torch.tensor(input_ids).to('cuda')  # Convert to tensor\n",
        "    attention_mask_tensor = (input_ids_tensor != tokenizer.pad_token_id).long()  # Create attention mask\n",
        "\n",
        "    for rsid_id in rsid_token_ids:\n",
        "        if rsid_id in input_ids_tensor:\n",
        "            masked_input_ids = input_ids_tensor.clone()\n",
        "            masked_input_ids[input_ids_tensor == rsid_id] = MASK_TOKEN\n",
        "            with torch.no_grad():\n",
        "                outputs = model(masked_input_ids.unsqueeze(0), attention_mask=attention_mask_tensor.unsqueeze(0))\n",
        "            embeddings.append(outputs.last_hidden_state[0, masked_input_ids == MASK_TOKEN].mean(dim=0).cpu().numpy())\n",
        "\n",
        "    # Step 2: Aggregate the embeddings (average them)\n",
        "    if embeddings:\n",
        "        average_embedding = np.mean(embeddings, axis=0)\n",
        "        sentence_embeddings[pmid] = average_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbHhXyg9aUFg",
        "outputId": "b688d3b7-f021-4a73-ea85-63260014dcf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 571/571 [53:26<00:00,  5.62s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the sentence embeddings to a file\n",
        "import pickle\n",
        "\n",
        "with open(\"sentence_embeddings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(sentence_embeddings, f)"
      ],
      "metadata": {
        "id": "e-hqCoc1HOKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Calculate cosine similarities with the embeddings from the rest of the corpus\n",
        "similarities = {}\n",
        "\n",
        "# Set the similarity threshold\n",
        "similarity_threshold = 0.98\n",
        "\n",
        "for pmid, embedding in tqdm(sentence_embeddings.items(), desc=\"Calculating similarities\"):\n",
        "    similarities[pmid] = {}\n",
        "    for other_pmid, other_embedding in sentence_embeddings.items():\n",
        "        if pmid != other_pmid:\n",
        "            similarity = cosine_similarity([embedding], [other_embedding])[0][0]\n",
        "            if similarity >= similarity_threshold:\n",
        "                similarities[pmid][other_pmid] = similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORY-3aHpo2Pk",
        "outputId": "6b38f886-4c78-4911-fb56-7356f8b8fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating similarities: 100%|██████████| 387/387 [00:35<00:00, 10.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Convert float32 values to float\n",
        "similarities_float = {\n",
        "    pmid: {pmid2: float(sim) for pmid2, sim in pmid_sim.items()}\n",
        "    for pmid, pmid_sim in similarities.items()\n",
        "}\n",
        "\n",
        "# Store results in a JSON file\n",
        "with open('similarities.json', 'w') as f:\n",
        "    json.dump(similarities_float, f)"
      ],
      "metadata": {
        "id": "gfg6IUhPJ1vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the similarities in descending order\n",
        "sorted_similarities = {pmid: sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)[:5] for pmid, similarity_dict in similarities_float.items()}\n",
        "\n",
        "# Print a sample of results\n",
        "sample_pmid = list(sorted_similarities.keys())[0]\n",
        "sample_results = {sample_pmid: sorted_similarities[sample_pmid]}\n",
        "\n",
        "print(json.dumps(sample_results, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXgSw7qZKEJe",
        "outputId": "a3207a99-2d28-4d24-f142-3c7087c2ca1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"36082566\": [\n",
            "        [\n",
            "            \"21304977\",\n",
            "            0.995354175567627\n",
            "        ],\n",
            "        [\n",
            "            \"35226426\",\n",
            "            0.9947450757026672\n",
            "        ],\n",
            "        [\n",
            "            \"11883940\",\n",
            "            0.9939204454421997\n",
            "        ],\n",
            "        [\n",
            "            \"12566567\",\n",
            "            0.993152916431427\n",
            "        ],\n",
            "        [\n",
            "            \"34482844\",\n",
            "            0.9931391477584839\n",
            "        ]\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"./sampled_data.pkl\" \"/content/gdrive/My Drive/Ver S/New/\"\n",
        "!cp \"./similarities.json\" \"/content/gdrive/My Drive/Ver S/New/\"\n",
        "!cp \"./sentence_embeddings.pkl\"  \"/content/gdrive/My Drive/Ver S/New/\""
      ],
      "metadata": {
        "id": "3ap-r49OK3Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store to SQL db"
      ],
      "metadata": {
        "id": "V1HEDyt5K3Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "nAbsabMUK29z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09d0314-54bf-4adb-a6aa-430384f56e9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method.\n",
        "\n",
        "\n",
        "1. Load `similarities.json` and `rsid_sentences.json` from the local directory.\n",
        "2. Extract sentences with multiple RSIDs.\n",
        "3. Cross-check the top RSIDs and RSID pairs with the similarity scores.\n",
        "4. Store the structured data (similarity scores associated with the top RSIDs and RSID pairs) in a database (in this example, we'll use SQLite for simplicity).\n",
        "\n"
      ],
      "metadata": {
        "id": "FlGW369pMsmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import sqlite3"
      ],
      "metadata": {
        "id": "4a0Mhi2SMmAv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/My Drive/Ver S/New/similarities.json\", \"r\") as file:\n",
        "    similarities_data = json.load(file)"
      ],
      "metadata": {
        "id": "3wQmsI3FM6rj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/My Drive/Ver S/rsid_sentences.json\", \"r\") as file:\n",
        "    rsid_sentences = {}\n",
        "    for line in file:\n",
        "        rsid_sentences.update(json.loads(line.strip()))"
      ],
      "metadata": {
        "id": "a0Zei8ilM8Zh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Extract sentences with multiple RSIDs\n",
        "rsid_pattern = re.compile(r'rs\\d+')\n",
        "multi_rsid_sentences = {key: [sentence for sentence in sentences if len(rsid_pattern.findall(sentence)) >= 2] for key, sentences in rsid_sentences.items() if any(len(rsid_pattern.findall(sentence)) >= 2 for sentence in sentences)}\n"
      ],
      "metadata": {
        "id": "zsYborNFNfuy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Cross-check the top RSID pairs (excluding pairs with identical RSIDs)\n",
        "filtered_rsid_pair_list = [pair for pair in top_rsid_pair_list if pair[0] != pair[1]]\n",
        "\n",
        "cross_checked_data = {}\n",
        "for key, sentences in multi_rsid_sentences.items():\n",
        "    for sentence in sentences:\n",
        "        for rsid_pair in filtered_rsid_pair_list:\n",
        "            if all(rs in sentence for rs in rsid_pair) and key in similarities_data:\n",
        "                if rsid_pair not in cross_checked_data:\n",
        "                    cross_checked_data[rsid_pair] = {}\n",
        "                cross_checked_data[rsid_pair][key] = similarities_data[key]\n"
      ],
      "metadata": {
        "id": "8WDQ8rAVNkRq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DB  store the structured data in a database\n",
        "conn = sqlite3.connect(\"/content/gdrive/My Drive/Ver S/New/similarities.db\")\n",
        "cursor = conn.cursor()\n",
        "cursor.execute('''CREATE TABLE IF NOT EXISTS similarities (PMID INTEGER, rsid_pair TEXT, similarity_score REAL)''')\n",
        "\n",
        "# Aggregate the data and compute the average similarity score\n",
        "aggregated_data = {}\n",
        "for rsid_pair, data in cross_checked_data.items():\n",
        "    for PMID, similar_sentences in data.items():\n",
        "        if (PMID, rsid_pair) not in aggregated_data:\n",
        "            aggregated_data[(PMID, rsid_pair)] = []\n",
        "        aggregated_data[(PMID, rsid_pair)].extend(similar_sentences.values())\n",
        "\n",
        "# Calculate the average for each group and insert into the database\n",
        "for (PMID, rsid_pair), scores in aggregated_data.items():\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    cursor.execute(\"INSERT INTO similarities (PMID, rsid_pair, similarity_score) VALUES (?, ?, ?)\", (int(PMID), str(rsid_pair), avg_score))\n",
        "\n",
        "# Order the data by PMID in ascending order\n",
        "cursor.execute(\"SELECT * FROM similarities ORDER BY PMID ASC\")\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"db save success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGaswA6pNkL6",
        "outputId": "3170f83b-d1da-41de-f473-006949ebe549"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "db save success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#End."
      ],
      "metadata": {
        "id": "z8_elhdEK22h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olwoBQmKN3N5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}